{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPUwOvgUyZiz"
      },
      "source": [
        "requiremenrs.txt\n",
        "\n",
        "langchain\n",
        "langchain-openai\n",
        "langchainhub # langchain python라이브러리로 프롬프트, 에이전트, 체인 관련 패키지 모음\n",
        "langserve[all]\n",
        "\n",
        "faiss-cpu  # Facebook에서 개발 및 배포한 밀집 벡터의 유사도 측정, 클러스터링에 효율적인 라이브러리\n",
        "tavily-python # 언어 모델에 중립적인 디자인으로, 모든 LLM과 통합이 가능하도록 설계된 검색 API\n",
        "beautifulsoup4  #파이썬에서 사용할 수 있는 웹데이터 크롤링 라이브러리\n",
        "wikipedia\n",
        "\n",
        "fastapi #  Python의 API를 빌드하기 위한 웹 프레임워크\n",
        "uvicorn # ASGI(Asynchronous Server Gateway Interface) 서버\n",
        "urllib3 # 파이썬에서 HTTP 요청을 보내고 받는 데 사용되는 강력하고 유연한 라이브러리\n",
        "\n",
        "python-dotenv\n",
        "pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMMJXo_JyjhQ"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-openai\n",
        "!pip install python-dotenv\n",
        "!pip install langchain_community\n",
        "!pip install pypdf\n",
        "!pip install faiss-cpu\n",
        "!pip install wikipedia\n",
        "!pip install openai\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXEvb3WyJMcA"
      },
      "source": [
        "Tavily Search 를 사용하기 위해서는 API KEY를 발급 받아 등록해야 함.\n",
        "\n",
        "[Tavily Search API 발급받기](https://app.tavily.com/sign-in)\n",
        "\n",
        "발급 받은 API KEY 를 다음과 같이 환경변수에 등록"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIxxUDEZI6ZR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# TAVILY API KEY를 기입합니다.\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
        "\n",
        "# 디버깅을 위한 프로젝트명을 기입합니다.\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"AGENT TUTORIAL\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys24Z3bfJHUf"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEii2SHNJbAG"
      },
      "outputs": [],
      "source": [
        "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# API KEY 정보로드\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezbT1NHQKP12"
      },
      "outputs": [],
      "source": [
        "#google drive load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKz2oCpl6lWK"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "import openai\n",
        "import unicodedata\n",
        "import json\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# ─── 0) OpenAI API 키 설정 ─────────────────────────────────────────────────\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# ─── 1) JSON 로드 + 메타데이터 추출 함수 ─────────────────────────────────────────\n",
        "def load_documents_with_metadata(folder_path):\n",
        "    documents = []\n",
        "    for raw_filename in os.listdir(folder_path):\n",
        "        filename = unicodedata.normalize(\"NFC\", raw_filename)\n",
        "        file_path = os.path.join(folder_path, raw_filename)\n",
        "\n",
        "        if not os.path.isfile(file_path):\n",
        "            continue\n",
        "        if not filename.endswith(\".json\"):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            parts = filename.replace(\".json\", \"\").split(\"_\")\n",
        "            emotion = parts[1] if len(parts) > 1 else \"unknown\"\n",
        "            relation = parts[2] if len(parts) > 2 else \"unknown\"\n",
        "\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "                utterances = data.get(\"utterances\", [])\n",
        "                full_text = \"\\n\".join([utt.get(\"text\",\"\") for utt in utterances])\n",
        "                if full_text.strip() == \"\":\n",
        "                    continue\n",
        "\n",
        "                doc = Document(\n",
        "                    page_content=full_text,\n",
        "                    metadata={\"filename\": filename, \"emotion\": emotion, \"relation\": relation}\n",
        "                )\n",
        "                documents.append(doc)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 오류 발생 ({filename}): {e}\")\n",
        "\n",
        "    return documents\n",
        "\n",
        "# ─── 2) 문서 분할 함수 ─────────────────────────────────────────────────────\n",
        "def split_documents(documents):\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "    return splitter.split_documents(documents)\n",
        "\n",
        "# ─── 3) FAISS 인덱스 생성 혹은 로드 함수 ───────────────────────────────────\n",
        "def create_or_load_faiss(index_dir, split_docs):\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    if os.path.isdir(index_dir) and os.path.exists(os.path.join(index_dir, \"index.faiss\")):\n",
        "        faiss_db = FAISS.load_local(index_dir, embeddings, allow_dangerous_deserialization=True)\n",
        "        print(\"✅ 기존 FAISS 인덱스를 로드했습니다.\")\n",
        "    else:\n",
        "        faiss_db = FAISS.from_documents(split_docs, embeddings)\n",
        "        os.makedirs(index_dir, exist_ok=True)\n",
        "        faiss_db.save_local(index_dir)\n",
        "        print(\"✅ 새로운 FAISS 인덱스를 생성하고 저장했습니다.\")\n",
        "    return faiss_db\n",
        "\n",
        "# ─── 4) 필터 + 유사도 검색 함수 ────────────────────────────────────────────────\n",
        "def filtered_similarity_search(vectorstore, query, emotion=None, relation=None, k=3):\n",
        "    all_docs = vectorstore.docstore._dict.values()\n",
        "    filtered_docs = [\n",
        "        doc for doc in all_docs\n",
        "        if (emotion is None or doc.metadata.get(\"emotion\") == emotion)\n",
        "        and (relation is None or relation in doc.metadata.get(\"relation\"))\n",
        "    ]\n",
        "\n",
        "    if not filtered_docs:\n",
        "        return []\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    query_chunks = splitter.split_text(query)\n",
        "\n",
        "    search_results = []\n",
        "    for chunk in query_chunks:\n",
        "        search_results.extend(vectorstore.similarity_search(chunk, k=k))\n",
        "    return search_results\n",
        "\n",
        "# ─── 5) 후보 중 최고 문서 선택 함수 ─────────────────────────────────────────────\n",
        "def choose_best_doc_with_gpt(query, docs, model=\"gpt-4o-mini\"):\n",
        "    prompt_parts = [\n",
        "        \"당신은 대화 응답 후보를 평가하는 전문가입니다.\\n\",\n",
        "        f\"사용자 질문: \\\"{query}\\\"\\n\",\n",
        "        \"다음은 검색된 응답 후보들입니다.\\n\"\n",
        "    ]\n",
        "\n",
        "    for idx, doc in enumerate(docs, start=1):\n",
        "        snippet = doc.page_content.strip().replace(\"\\n\", \" \")\n",
        "        if len(snippet) > 300:\n",
        "            snippet = snippet[:300] + \"...\"\n",
        "        prompt_parts.append(\n",
        "            f\"[{idx}]\\n\"\n",
        "            f\"Filename: {doc.metadata.get('filename')}\\n\"\n",
        "            f\"Emotion: {doc.metadata.get('emotion')}, Relation: {doc.metadata.get('relation')}\\n\"\n",
        "            f\"Content: \\\"{snippet}\\\"\\n\"\n",
        "        )\n",
        "\n",
        "    prompt_parts.append(\n",
        "        \"\\n위 후보들 중에서, 사용자 질문에 가장 적절한 응답을 하나 선택하고, 그 이유를 간단히 설명해주세요.\\n\"\n",
        "        \"반드시 다음 형식으로 응답해 주세요:\\n\"\n",
        "        \"선택: [번호]\\n\"\n",
        "        \"이유: [간단한 설명]\\n\"\n",
        "    )\n",
        "\n",
        "    full_prompt = \"\\n\".join(prompt_parts)\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"당신은 뛰어난 대화 평가자입니다.\"},\n",
        "            {\"role\": \"user\", \"content\": full_prompt}\n",
        "        ],\n",
        "        max_tokens=300,\n",
        "        temperature=0.0\n",
        "    )\n",
        "\n",
        "    gpt_reply = response.choices[0].message.content.strip()\n",
        "    selected_idx = None\n",
        "    for line in gpt_reply.splitlines():\n",
        "        if line.strip().startswith(\"선택\"):\n",
        "            import re\n",
        "            m = re.search(r\"\\[(\\d+)\\]\", line)\n",
        "            if m:\n",
        "                selected_idx = int(m.group(1))\n",
        "                break\n",
        "\n",
        "    if selected_idx is None or selected_idx < 1 or selected_idx > len(docs):\n",
        "        selected_idx = 1\n",
        "\n",
        "    best_doc = docs[selected_idx - 1]\n",
        "    return best_doc, gpt_reply\n",
        "\n",
        "# ─── 6) 최종 답변 간결하게 생성 함수 ─────────────────────────────────────────────\n",
        "def generate_final_answer(query, best_doc, model=\"gpt-4o-mini\"):\n",
        "    prompt = (\n",
        "        \"다음은 사용자의 질문과, 선택된 최적 응답 후보입니다.\\n\\n\"\n",
        "        f\"사용자 질문: \\\"{query}\\\"\\n\"\n",
        "        \"선택된 후보 응답 내용(원문):\\n\"\n",
        "        f\"\\\"\\\"\\\"\\n{best_doc.page_content}\\n\\\"\\\"\\\"\\n\\n\"\n",
        "        \"위 원문에서, 불필요한 반복/인사말/개인정보 등은 모두 제거하고, \"\n",
        "        \"사용자가 이해하기 쉽도록 핵심만 남겨 간결하게 재작성해주세요.\\n\"\n",
        "        \"문체는 친절하고 공감 가득한 톤을 유지해 주시고, \"\n",
        "        \"최종 답변만 출력해 주세요.\"\n",
        "    )\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"당신은 친절하고 공감능력이 뛰어난 상담사입니다.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=300,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    final_answer = response.choices[0].message.content.strip()\n",
        "    return final_answer\n",
        "\n",
        "# ─── 7) Gradio 응용: 채팅 인터페이스 구축 ───────────────────────────────────────\n",
        "index_dir = \"/content/drive/MyDrive/2025_Bigdata_nlp_class/faiss_index\"\n",
        "folder_path = \"/content/drive/MyDrive/2025_Bigdata_nlp_class/aihub_dataset/Training/02_label_data\"\n",
        "\n",
        "# 문서 로드 및 FAISS 초기화\n",
        "documents = load_documents_with_metadata(folder_path)\n",
        "split_docs = split_documents(documents)\n",
        "faiss_db = create_or_load_faiss(index_dir, split_docs)\n",
        "\n",
        "def chat_response(query, emotion, relation):\n",
        "    candidates = filtered_similarity_search(faiss_db, query, emotion, relation)\n",
        "    if not candidates:\n",
        "        return \"조건에 맞는 문서가 없습니다.\"\n",
        "\n",
        "    best_doc, _ = choose_best_doc_with_gpt(query, candidates, model=\"gpt-4o-mini\")\n",
        "    final_answer = generate_final_answer(query, best_doc, model=\"gpt-4o-mini\")\n",
        "    return final_answer\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 감정/관계 기반 Empathy QA 시스템\")\n",
        "    with gr.Row():\n",
        "        txt_query = gr.Textbox(label=\"질문\", placeholder=\"질문을 입력하세요...\", lines=2)\n",
        "    with gr.Row():\n",
        "        txt_emotion = gr.Textbox(label=\"Emotion (예: 기쁨, 당황, 분노)\", placeholder=\"ex) 기쁨\")\n",
        "        txt_relation = gr.Textbox(label=\"Relation (예: 부모자녀, 부부, 연인)\", placeholder=\"ex) 부모자녀\")\n",
        "    btn_submit = gr.Button(\"전송\")\n",
        "    output = gr.Textbox(label=\"답변\", lines=5)\n",
        "\n",
        "    btn_submit.click(chat_response, inputs=[txt_query, txt_emotion, txt_relation], outputs=output)\n",
        "\n",
        "demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
