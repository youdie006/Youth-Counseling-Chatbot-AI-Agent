"""
OpenAI API ë¼ìš°í„°
GPT-4 ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸ë“¤ (ì±„íŒ…, ê°ì •ë¶„ì„ ë“±)
"""

from fastapi import APIRouter, HTTPException, Depends, status
from typing import List, Dict, Any
from loguru import logger

from ..services.openai_client import get_openai_client
from ..models.function_models import (
    OpenAICompletionRequest, OpenAICompletionResponse,
    EmotionAnalysisRequest, EmotionAnalysisResponse,
    ChatMessage, SystemHealthCheck
)


router = APIRouter()


@router.post("/completion", response_model=OpenAICompletionResponse)
async def create_completion(
    request: OpenAICompletionRequest,
    openai_client = Depends(get_openai_client)
):
    """
    ğŸ¤– GPT-4 ì±„íŒ… ì™„ì„± ìƒì„±

    - ì¼ë°˜ì ì¸ GPT-4 ì±„íŒ… ì™„ì„±
    - ì‚¬ìš©ì ì •ì˜ ëª¨ë¸, ì˜¨ë„, í† í° ìˆ˜ ì„¤ì • ê°€ëŠ¥
    - ìŠ¤íŠ¸ë¦¬ë° ì§€ì› (ì„ íƒì )
    """
    try:
        logger.info(f"GPT-4 ì™„ì„± ìš”ì²­ - ëª¨ë¸: {request.model}, ë©”ì‹œì§€ ìˆ˜: {len(request.messages)}")

        # ChatMessageë¥¼ dictë¡œ ë³€í™˜
        messages = [
            {"role": msg.role.value, "content": msg.content}
            for msg in request.messages
        ]

        response = await openai_client.create_completion(
            messages=messages,
            model=request.model,
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            top_p=request.top_p
        )

        return response

    except Exception as e:
        logger.error(f"GPT-4 ì™„ì„± ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"GPT-4 ì™„ì„± ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.post("/teen-empathy", response_model=str)
async def create_teen_empathy_response(
    user_message: str,
    conversation_history: List[ChatMessage] = None,
    context_info: str = None,
    openai_client = Depends(get_openai_client)
):
    """
    ğŸ’™ ì²­ì†Œë…„ ê³µê°í˜• ì‘ë‹µ ìƒì„±

    - ì²­ì†Œë…„ ì „ìš© ê³µê° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì ìš©
    - ëŒ€í™” íˆìŠ¤í† ë¦¬ ë° ë§¥ë½ ì •ë³´ í™œìš©
    - ë”°ëœ»í•˜ê³  ì§€ì§€ì ì¸ ì‘ë‹µ ìƒì„±
    """
    try:
        logger.info(f"ì²­ì†Œë…„ ê³µê° ì‘ë‹µ ìš”ì²­: '{user_message[:50]}...'")

        response = await openai_client.create_teen_empathy_response(
            user_message=user_message,
            conversation_history=conversation_history,
            context_info=context_info
        )

        return response

    except Exception as e:
        logger.error(f"ì²­ì†Œë…„ ê³µê° ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì²­ì†Œë…„ ê³µê° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.post("/analyze-emotion", response_model=EmotionAnalysisResponse)
async def analyze_emotion(
    request: EmotionAnalysisRequest,
    openai_client = Depends(get_openai_client)
):
    """
    ğŸ­ ê°ì • ë° ë§¥ë½ ë¶„ì„

    - í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ê°ì • ì¶”ì¶œ
    - ê´€ê³„ ë§¥ë½ íŒŒì•… (ë¶€ëª¨ë‹˜, ì¹œêµ¬, í˜•ì œìë§¤ ë“±)
    - ì ì ˆí•œ ê³µê° ì „ëµ ì¶”ì²œ
    """
    try:
        logger.info(f"ê°ì • ë¶„ì„ ìš”ì²­: '{request.text[:50]}...'")

        response = await openai_client.analyze_emotion_and_context(
            text=request.text,
            additional_context=request.context
        )

        return response

    except Exception as e:
        logger.error(f"ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.post("/react-response")
async def generate_react_response(
    user_message: str,
    similar_contexts: List[Dict[str, Any]] = None,
    emotion: str = None,
    relationship: str = None,
    openai_client = Depends(get_openai_client)
):
    """
    ğŸ§  ReAct íŒ¨í„´ ì‘ë‹µ ìƒì„±

    - Thought â†’ Action â†’ Observation â†’ Response
    - ë‹¨ê³„ë³„ ì¶”ë¡  ê³¼ì • í¬í•¨
    - ìœ ì‚¬ ë§¥ë½ ì •ë³´ í™œìš©
    """
    try:
        logger.info(f"ReAct ì‘ë‹µ ìš”ì²­: '{user_message[:50]}...'")

        response_text, react_steps = await openai_client.generate_react_response(
            user_message=user_message,
            similar_contexts=similar_contexts or [],
            emotion=emotion,
            relationship=relationship
        )

        return {
            "response": response_text,
            "react_steps": react_steps,
            "metadata": {
                "emotion": emotion,
                "relationship": relationship,
                "context_count": len(similar_contexts) if similar_contexts else 0
            }
        }

    except Exception as e:
        logger.error(f"ReAct ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ReAct ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get("/models")
async def list_available_models():
    """
    ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡

    - ì§€ì›í•˜ëŠ” OpenAI ëª¨ë¸ë“¤
    - ê° ëª¨ë¸ì˜ íŠ¹ì§• ë° ì‚¬ìš© ê¶Œì¥ì‚¬í•­
    """
    return {
        "available_models": [
            {
                "name": "gpt-4",
                "description": "ê°€ì¥ ê°•ë ¥í•œ ëª¨ë¸, ë³µì¡í•œ ì¶”ë¡ ì— ìµœì ",
                "recommended_for": ["ì²­ì†Œë…„ ê³µê° ìƒë‹´", "ë³µì¡í•œ ë§¥ë½ ì´í•´"],
                "max_tokens": 8192,
                "cost": "ë†’ìŒ"
            },
            {
                "name": "gpt-4-turbo",
                "description": "ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ GPT-4 ë²„ì „",
                "recommended_for": ["ì‹¤ì‹œê°„ ì±„íŒ…", "ì¼ë°˜ì ì¸ ìƒë‹´"],
                "max_tokens": 128000,
                "cost": "ì¤‘ê°„"
            },
            {
                "name": "gpt-3.5-turbo",
                "description": "ë¹ ë¥´ê³  ê²½ì œì ì¸ ëª¨ë¸",
                "recommended_for": ["ê°„ë‹¨í•œ ì§ˆë¬¸", "í…ŒìŠ¤íŠ¸ìš©"],
                "max_tokens": 4096,
                "cost": "ë‚®ìŒ"
            }
        ],
        "current_default": "gpt-4",
        "recommendation": "ì²­ì†Œë…„ ê³µê°í˜• ìƒë‹´ì—ëŠ” gpt-4ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤"
    }


@router.get("/health", response_model=SystemHealthCheck)
async def openai_health_check(openai_client = Depends(get_openai_client)):
    """
    ğŸ’Š OpenAI ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬

    - API ì—°ê²° ìƒíƒœ í™•ì¸
    - ì‘ë‹µ ì‹œê°„ ì¸¡ì •
    - ì„œë¹„ìŠ¤ ê°€ìš©ì„± ì ê²€
    """
    try:
        import time
        start_time = time.time()

        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­ìœ¼ë¡œ ì—°ê²° í™•ì¸
        test_response = await openai_client.create_completion(
            messages=[{"role": "user", "content": "Hello"}],
            max_tokens=5,
            temperature=0
        )

        response_time_ms = (time.time() - start_time) * 1000

        return SystemHealthCheck(
            status="healthy",
            services={
                "openai_api": True,
                "gpt4_model": True,
                "embedding_generation": True
            },
            response_time_ms=response_time_ms,
            version="1.0.0"
        )

    except Exception as e:
        logger.error(f"OpenAI í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {e}")
        return SystemHealthCheck(
            status="unhealthy",
            services={
                "openai_api": False,
                "gpt4_model": False,
                "embedding_generation": False
            },
            response_time_ms=0.0,
            version="1.0.0"
        )


@router.get("/usage-stats")
async def get_usage_stats():
    """
    ğŸ“Š OpenAI API ì‚¬ìš© í†µê³„

    - í† í° ì‚¬ìš©ëŸ‰ ì¶”ì •
    - ë¹„ìš© ê´€ë ¨ ì •ë³´
    """
    return {
        "current_session": {
            "requests_made": "ì‹¤ì‹œê°„ ì¶”ì  í•„ìš”",
            "tokens_used": "ì‹¤ì‹œê°„ ì¶”ì  í•„ìš”",
            "estimated_cost": "ì‹¤ì‹œê°„ ì¶”ì  í•„ìš”"
        },
        "cost_info": {
            "gpt-4": {
                "input_per_1k_tokens": "$0.03",
                "output_per_1k_tokens": "$0.06"
            },
            "gpt-4-turbo": {
                "input_per_1k_tokens": "$0.01",
                "output_per_1k_tokens": "$0.03"
            },
            "gpt-3.5-turbo": {
                "input_per_1k_tokens": "$0.0015",
                "output_per_1k_tokens": "$0.002"
            }
        },
        "optimization_tips": [
            "ì ì ˆí•œ max_tokens ì„¤ì •ìœ¼ë¡œ ë¹„ìš© ì ˆì•½",
            "ê°„ë‹¨í•œ ì‘ì—…ì€ gpt-3.5-turbo ì‚¬ìš©",
            "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìµœì í™”ë¡œ í† í° ì ˆì•½",
            "ë¶ˆí•„ìš”í•œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì œê±°"
        ]
    }


@router.post("/test-empathy")
async def test_empathy_response(
    test_message: str = "ì¹œêµ¬ê°€ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ê¸°ë¶„ì´ ë‚˜ë¹ ",
    openai_client = Depends(get_openai_client)
):
    """
    ğŸ§ª ê³µê°í˜• ì‘ë‹µ í…ŒìŠ¤íŠ¸

    - ì²­ì†Œë…„ ê³µê°í˜• ì‹œìŠ¤í…œì˜ ì‘ë‹µ í’ˆì§ˆ í…ŒìŠ¤íŠ¸
    - ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì œê³µ
    """
    try:
        # ê°ì • ë¶„ì„
        emotion_result = await openai_client.analyze_emotion_and_context(test_message)

        # ê³µê°í˜• ì‘ë‹µ ìƒì„±
        empathy_response = await openai_client.create_teen_empathy_response(test_message)

        # ReAct ì‘ë‹µ ìƒì„±
        react_response, react_steps = await openai_client.generate_react_response(
            user_message=test_message,
            emotion=emotion_result.primary_emotion.value,
            relationship=emotion_result.relationship_context.value if emotion_result.relationship_context else None
        )

        return {
            "test_input": test_message,
            "emotion_analysis": {
                "primary_emotion": emotion_result.primary_emotion.value,
                "confidence": emotion_result.emotion_confidence,
                "relationship": emotion_result.relationship_context.value if emotion_result.relationship_context else None,
                "strategies": [s.value for s in emotion_result.recommended_strategies]
            },
            "empathy_response": empathy_response,
            "react_response": {
                "response": react_response,
                "steps": react_steps
            },
            "test_info": {
                "response_quality": "ìˆ˜ë™ í‰ê°€ í•„ìš”",
                "empathy_level": "ìˆ˜ë™ í‰ê°€ í•„ìš”",
                "actionability": "ìˆ˜ë™ í‰ê°€ í•„ìš”"
            }
        }

    except Exception as e:
        logger.error(f"ê³µê° ì‘ë‹µ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )